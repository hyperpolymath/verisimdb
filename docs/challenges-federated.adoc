// SPDX-License-Identifier: PMPL-1.0-or-later

= Challenges: Federated Deployment Mode

== Overview

Federated deployment runs VeriSimDB as a **tiny coordinator** (<5k LOC ReScript + Elixir) while modality stores are **distributed across independent institutions**. This mode enables cross-institutional collaboration and data sovereignty but introduces complex challenges in consensus, trust, and distributed coordination.

This document details the **technical challenges** unique to federated deployment, focusing on the coordination layer and cross-organizational boundaries.

== 1. Consensus and Coordination Challenges

=== 1.1. KRaft Metadata Consensus Overhead

**Problem:** Every registry update (Hexad registration, policy change) requires **Raft quorum consensus**.

**Raft Requirements:**
- **Minimum Nodes:** 3 (tolerates 1 failure)
- **Recommended:** 5 (tolerates 2 failures)
- **Quorum:** ⌈N/2⌉ + 1 nodes must acknowledge

**Latency Impact:**
[source,text]
----
Single-Node Write (Standalone): 1-5ms (local disk)
Raft Consensus Write (Federated): 50-500ms depending on geography

Breakdown:
  - Leader receives write request: 0ms
  - Leader → Follower 1 (US East → US West): 70ms
  - Leader → Follower 2 (US East → EU): 100ms
  - Followers write to log: 10ms each
  - Followers → Leader ACK: 100ms (slowest path)
  - Leader commits and responds: 180ms total
----

**Throughput Limit:**
- **Theoretical:** ~1000 writes/sec per Raft group
- **Practical:** ~100-500 writes/sec with cross-region latency

**Consequences:**
- **High Latency:** Registry updates 50-100x slower than standalone
- **Write Bottleneck:** Cannot scale writes beyond single Raft group capacity
- **Complexity:** Raft log compaction, snapshot management required

**Mitigation:**
- **Batching:** Batch multiple registry updates into single Raft entry
- **Regional Sharding:** Multiple Raft groups per region (but: adds complexity)
- **Read-Heavy Optimization:** Serve reads from any replica (eventual consistency acceptable)
- **Log Compaction:** Snapshot registry state every N entries, truncate old log

=== 1.2. Split-Brain and Network Partitions

**Problem:** Network partition can split Raft quorum, causing **availability vs consistency tradeoff**.

**Partition Scenarios:**

**Scenario 1: Majority Partition**
[source,text]
----
Initial: 5 nodes (Leader + 4 Followers)
Partition: [Leader, F1, F2] | [F3, F4]

Majority partition (3 nodes): ✓ Can commit writes
Minority partition (2 nodes): ✗ Cannot elect leader, read-only

Result: Majority partition remains available, minority unavailable
----

**Scenario 2: No Majority Partition**
[source,text]
----
Initial: 5 nodes
Partition: [L, F1] | [F2, F3] | [F4]

No partition has 3 nodes → ✗ No writes possible
All partitions read-only until partition heals

Result: Total write unavailability
----

**Real-World Example:**
- University A (US East) - Leader
- University B (US West) - Follower
- University C (EU) - Follower
- University D (Asia) - Follower
- University E (AU) - Follower

**If EU ↔ US link fails:** US+Asia+AU (3 nodes) continue, EU isolated

**Consequences:**
- **Prolonged Partitions:** Can last hours-to-days for undersea cable failures
- **Data Divergence:** Minority partition accepts reads of stale data
- **Manual Intervention:** May require admin intervention to resolve

**Mitigation:**
- **Monitoring:** Detect partitions quickly via heartbeat monitoring
- **Graceful Degradation:** Serve stale data with **staleness indicator** (e.g., "data as of 2 hours ago")
- **Multi-Region Raft:** Deploy separate Raft groups per region, federate at higher level
- **Automated Failover:** Use **Raft pre-vote** to prevent split elections

=== 1.3. Leader Election Storms

**Problem:** Frequent leader failures cause **election cascades**, reducing availability.

**Election Storm Trigger:**
1. Leader node becomes overloaded (CPU 100%)
2. Heartbeat messages delayed → Followers timeout
3. Followers start elections simultaneously
4. Multiple candidates split votes → No leader elected
5. Repeat until timeout increases or load decreases

**Duration:** Each election round takes **election timeout** (typically 150-300ms) × **retry attempts**

**Impact Example:**
[source,text]
----
Scenario: Leader overload + 3 failed elections

T=0:     Leader misses heartbeat
T=200ms: Follower 1 starts election
T=210ms: Follower 2 starts election (vote split)
T=400ms: Timeout, retry election
T=600ms: Timeout, retry election
T=800ms: Finally elect leader

Result: 800ms write unavailability
----

**Consequences:**
- **Write Downtime:** All writes blocked during election
- **Cascading Failures:** If new leader also overloaded, repeat
- **Client Confusion:** Clients retry writes, amplifying load

**Mitigation:**
- **Pre-Vote Phase:** Raft extension where candidates check viability before election
- **Leader Stickiness:** Increase heartbeat frequency, decrease election timeout variance
- **Load Shedding:** Leader sheds non-critical load when CPU > 80%
- **Backup Leaders:** Designate "preferred leader" based on network centrality

== 2. Trust and Security Challenges

=== 2.1. Multi-Party Signature Verification

**Problem:** Every federated store must **sign** its data; coordinator must **verify** all signatures.

**Signature Overhead:**
[source,text]
----
Operation: Query Hexad across 3 federated stores

1. Orchestrator sends query to Store A, B, C (parallel)
2. Each store:
   - Fetches data (10ms)
   - Computes signature with private key (5ms)
   - Returns {data, signature}
3. Orchestrator verifies signatures:
   - Fetch Store A public key from registry (cached: 1ms)
   - Verify signature A (10ms)
   - Verify signature B (10ms)
   - Verify signature C (10ms)
4. Merge results

Total overhead: 30ms signature verification (vs 0ms standalone)
----

**Cryptographic Load:**
- **Ed25519 Verification:** ~10ms per signature (single-core)
- **At Scale:** 100 federated stores × 1000 qps = **100k signature verifications/sec**
- **CPU Requirement:** ~10 cores dedicated to signature verification

**Consequences:**
- **Latency Tax:** Every query pays 30-50ms signature overhead
- **CPU Intensive:** Orchestrator needs powerful CPUs (not I/O bound like standalone)
- **Key Rotation Complexity:** Must coordinate public key updates across all stores

**Mitigation:**
- **Batch Verification:** Verify multiple signatures in parallel (GPU acceleration)
- **Caching:** Cache verification results for **immutable data** (e.g., historical versions)
- **Signature Aggregation:** Use BLS signatures to aggregate N signatures into 1
- **Hardware Acceleration:** Use AES-NI/AVX2 instructions for crypto operations

=== 2.2. Byzantine Fault Tolerance

**Problem:** Federated stores may be **malicious** (send incorrect data, violate policies).

**Threat Model:**
[cols="1,2,2"]
|===
|Attack |Example |Impact

|**Data Tampering**
|Store A returns modified Hexad content
|Clients receive incorrect data

|**Policy Violation**
|Store B grants access despite policy denying it
|Unauthorized data access

|**Availability Attack**
|Store C always times out (DoS)
|Query failures, degraded UX

|**Sybil Attack**
|Attacker registers 10 fake stores, controls majority
|Registry poisoning
|===

**Byzantine Generals Problem:**
- **Classic Raft:** Assumes crash failures only (nodes fail-stop)
- **Federated VeriSimDB:** Must tolerate **byzantine failures** (malicious nodes)
- **BFT Requirement:** Need **3f + 1** nodes to tolerate **f** byzantine nodes

**Example:**
- To tolerate 1 byzantine store: Need 4 stores minimum
- To tolerate 2 byzantine stores: Need 7 stores minimum

**Consequences:**
- **Higher Overhead:** More replicas needed than crash-tolerant Raft
- **Complex Verification:** Must cross-check responses from multiple stores
- **Trust Assumptions:** Cannot assume federated stores are honest

**Mitigation:**
- **BFT Consensus:** Replace Raft with **Tendermint** or **HotStuff** for metadata log
- **Merkle Proofs:** Require stores to provide **Merkle proofs** for all data
- **Reputation System:** Track store reliability, downrank misbehaving stores
- **ZKP Verification:** Use `proven` ZKP to verify **logical correctness** without trusting store

=== 2.3. Trust Boundary Explosion

**Problem:** In federated mode, **every store is a trust boundary** → N stores = N boundaries.

**Trust Boundaries:**
[source,text]
----
Standalone: 1 boundary (orchestrator → local stores)
Hybrid: 3-5 boundaries (orchestrator → local + 2-3 remote)
Federated: 50-100 boundaries (orchestrator → N remote stores)
----

**Security Implications:**
1. **Compromise Surface:** Any one store compromise exposes its hosted Hexads
2. **Policy Inconsistency:** 100 stores = 100 different security policies to audit
3. **Credential Management:** Must manage 100 pairs of TLS certificates
4. **Audit Complexity:** Must correlate logs across 100 independent systems

**Example Breach Scenario:**
[source,text]
----
Attacker compromises University B's federated store:
  - Gains access to all Hexads hosted by University B
  - Can serve malicious data to orchestrator
  - Can exfiltrate query patterns (who queries what)

Impact: Partial breach (only University B's Hexads), but:
  - Orchestrator cannot detect tampering without cross-validation
  - Clients unaware they received compromised data
----

**Mitigation:**
- **Zero-Trust Architecture:** Verify **every** response from **every** store (never trust by default)
- **Defense in Depth:** Multiple layers of verification (signatures + Merkle proofs + ZKPs)
- **Continuous Monitoring:** Real-time anomaly detection for store behavior
- **Incident Response Plan:** Pre-negotiated procedures for store compromise

== 3. Operational Complexity

=== 3.1. Heterogeneous Store Management

**Problem:** Federated stores run **different software versions**, **different configurations**, and **different hardware**.

**Heterogeneity Matrix:**
[cols="1,2,2,2"]
|===
|Store |Software |Hardware |Network

|**University A**
|VeriSim 0.9 (Rust 1.70)
|32-core, 128GB RAM
|10 Gbps

|**Archive.org**
|Custom fork v0.8
|16-core, 64GB RAM (shared)
|1 Gbps (rate-limited)

|**Lab B**
|VeriSim 1.0-beta (Rust 1.75)
|8-core, 32GB RAM
|100 Mbps

|**Community Node**
|VeriSim 0.7 (outdated)
|4-core, 16GB RAM
|50 Mbps DSL
|===

**Consequences:**
- **API Incompatibility:** Old stores may not support new API features
- **Performance Variance:** 200x difference between fastest and slowest store
- **Bug Divergence:** Custom forks may have unique bugs
- **Upgrade Coordination:** Cannot force-upgrade 100 independent stores

**Mitigation:**
- **Versioned APIs:** Support multiple API versions in orchestrator (v1, v2, v3)
- **Capability Negotiation:** Stores advertise capabilities in registration manifest
- **Graceful Degradation:** If store doesn't support feature, skip that modality
- **Minimum Version Policy:** Require stores to be within N versions of current

=== 3.2. Cross-Institutional SLAs

**Problem:** Must coordinate **service level agreements** across independent organizations.

**SLA Negotiation Challenges:**
[cols="1,2,2"]
|===
|Aspect |Challenge |Example Conflict

|**Uptime**
|Different institutions have different availability requirements
|University A: 99.9% SLA, Archive.org: 95% SLA

|**Latency**
|Geographic distribution causes unavoidable latency variance
|US-EU queries: 100ms, US-Asia: 200ms

|**Data Retention**
|Legal requirements differ by jurisdiction
|EU GDPR: delete after 30 days, US: retain indefinitely

|**Maintenance Windows**
|Timezones make coordination difficult
|University A: 2am ET, Lab B: 2am PT (3-hour overlap)

|**Cost Sharing**
|Who pays for bandwidth, storage, compute?
|Free tier vs paid tier stores
|===

**Real-World Scenario:**
[source,text]
----
Query touches 5 stores:
  - 3 stores respond in 50ms (99.9% uptime)
  - 1 store responds in 500ms (99% uptime, overloaded)
  - 1 store times out (maintenance window)

Result: Query returns incomplete data with 500ms latency
User Experience: Slow and unreliable
----

**Mitigation:**
- **SLA Tiers:** Define Bronze/Silver/Gold tiers with clear expectations
- **Circuit Breakers:** Automatically exclude stores violating SLAs
- **Best-Effort Model:** Accept that federated queries are **inherently best-effort**
- **Contractual Agreements:** Formal MoUs between institutions (but: hard to enforce)

=== 3.3. Drift Detection Across Organizational Boundaries

**Problem:** Drift detection in federated mode requires **polling remote stores** → network overhead and coordination challenges.

**Drift Detection Protocol:**
[source,text]
----
1. Orchestrator queries all stores hosting Hexad 550e8400-...
   - Store A (Graph): GET /hexad/550e8400/graph/metadata
   - Store B (Vector): GET /hexad/550e8400/vector/metadata
   - Store C (Document): GET /hexad/550e8400/document/metadata

2. Compare metadata (timestamps, hashes, version numbers)

3. If drift detected:
   a. Determine authoritative source (policy-driven)
   b. Trigger repair:
      - Option 1: Pull from authoritative → Push to drifted stores
      - Option 2: Flag for manual review (if conflict)

4. Record drift event in Temporal log
----

**Challenges:**
- **Polling Overhead:** Must poll N stores every T minutes (N×T network requests)
- **Consistency Semantics:** Different stores may have different "freshness" guarantees
- **Repair Latency:** Cross-institutional repair can take hours (requires approval)
- **Policy Conflicts:** Authoritative source may disagree across institutions

**Example Drift Scenario:**
[source,text]
----
T=0:   Research paper retracted at University A
T=60:  University A updates Graph modality (removes citation edges)
T=120: Drift detection discovers Archive.org Document still references paper
T=180: Orchestrator sends repair request to Archive.org
T=240: Archive.org admin reviews request (manual process)
T=480: Archive.org applies update (8 hours later)

Reality: Cross-institutional drift repair is **not real-time**
----

**Mitigation:**
- **Event-Driven Updates:** Stores **push** change notifications instead of polling
- **Asynchronous Repair:** Accept that drift is **eventually** resolved (not immediately)
- **Priority Tiers:** Critical drift (e.g., retractions) gets expedited review
- **Automated Repair Policies:** Pre-negotiated rules for common drift cases

=== 3.4. Monitoring and Observability Gaps

**Problem:** Cannot directly access **logs**, **metrics**, or **traces** from federated stores.

**Observability Challenges:**
[cols="1,2,2"]
|===
|Data Type |Standalone |Federated

|**Logs**
|Direct access via journalctl/syslog
|Must request logs from store operator (may refuse)

|**Metrics**
|Prometheus scrape from localhost
|Must expose /metrics endpoint (may not implement)

|**Traces**
|OpenTelemetry spans in single system
|Distributed tracing requires cooperation (may not support)

|**Debugging**
|Can attach debugger, inspect memory
|Cannot access remote store internals
|===

**Example Debug Scenario:**
[source,text]
----
Problem: Queries to Store X are slow (500ms vs expected 50ms)

Standalone:
  - Check CPU, memory, disk I/O on local machine
  - Profile code with perf/flamegraph
  - Fix in minutes

Federated:
  - Email Store X operator: "Can you check if your system is slow?"
  - Wait 24 hours for response
  - Operator says: "Looks fine on our end"
  - Add synthetic monitoring, discover issue is network routing
  - Cannot fix (requires Store X ISP intervention)

Resolution time: Days to weeks
----

**Mitigation:**
- **Federated Observability Contract:** Require stores to export metrics/logs to central collector
- **Synthetic Monitoring:** Run continuous health checks from orchestrator to all stores
- **Anomaly Detection:** Use ML to detect outliers (e.g., Store X consistently 10x slower)
- **SLA Enforcement:** Downrank or exclude stores that don't provide observability data

== 4. Performance and Scaling Challenges

=== 4.1. Network Latency Dominates

**Problem:** In federated mode, **network latency** overwhelms compute time.

**Latency Breakdown:**
[source,text]
----
Standalone Query (Vector search):
  - Orchestrator → Local Vector store: 0.1ms (localhost)
  - HNSW search: 2ms
  - Return: 0.1ms
  Total: 2.2ms

Federated Query (Vector search):
  - Orchestrator → Remote Store X: 50ms (cross-country)
  - HNSW search: 2ms
  - Return: 50ms
  Total: 102ms (46x slower)

Cross-Region Query:
  - US → EU: 100ms RTT
  - Total: 202ms (92x slower)
----

**Consequences:**
- **Tail Latency:** p99 latency determined by **slowest store** in query path
- **Cannot Optimize Compute:** Optimizing HNSW from 2ms → 1ms irrelevant when network is 100ms
- **Geographic Penalty:** Queries spanning continents inherently slow

**Mitigation:**
- **CDN-Style Caching:** Cache hot data near orchestrator (stale acceptable for some use cases)
- **Regional Affinity:** Route queries to geographically close stores when possible
- **Speculative Execution:** Query multiple stores in parallel, use fastest response
- **Accept Latency:** Federated mode trades latency for **data sovereignty** and **institutional independence**

=== 4.2. Fan-Out Queries and Amplification

**Problem:** Cross-modal queries **fan out** to multiple stores, amplifying latency and failures.

**Fan-Out Example:**
[source,text]
----
Query: "Find papers similar to embedding X that cite paper Y"

Requires:
  1. Vector search (Store A)
  2. Graph traversal (Store B)
  3. Join results (orchestrator)

Sequential: 50ms + 50ms = 100ms
Parallel: max(50ms, 50ms) = 50ms

With N modalities: latency scales with max(Store1, Store2, ..., StoreN)
----

**Amplification Factors:**
- **2 modalities:** 2x network calls
- **6 modalities:** 6x network calls (if not optimized)
- **Failure Probability:** P(any store fails) = 1 - ∏(1 - P(store_i fails))

**Example:**
- Each store: 99% availability
- Query touches 6 stores
- Query success probability: 0.99^6 = **94%** (6% failure rate!)

**Consequences:**
- **High Failure Rate:** Multi-store queries less reliable than single-store
- **Partial Results:** Must handle cases where some stores timeout
- **Retry Storms:** Clients retry failed queries → amplify load on stores

**Mitigation:**
- **Query Planning:** Optimize to minimize store fan-out
- **Hedged Requests:** Send duplicate queries to backup stores if primary slow
- **Partial Result Tolerance:** Return best-effort results with **completeness indicator**
- **Circuit Breakers:** Fail fast if primary store down, avoid waiting for timeout

=== 4.3. Write Propagation Latency

**Problem:** Writes in federated mode must **propagate across stores**, introducing delays.

**Write Propagation Flow:**
[source,text]
----
T=0:   Client writes Hexad update to orchestrator
T=1:   Orchestrator commits to KRaft metadata log (quorum: 100ms)
T=101: Orchestrator dispatches writes to federated stores:
       - Store A (Graph): 50ms to reach, 10ms to persist
       - Store B (Vector): 80ms to reach, 5ms to persist
       - Store C (Document): 120ms to reach, 20ms to persist
T=241: All stores ACK write completion

Total write latency: 241ms (vs 5ms standalone)
----

**Consistency Implications:**
- **Eventual Consistency:** Reads during T=101 to T=241 see **incomplete data**
- **Read-After-Write Anomaly:** Client writes, immediately reads, sees old data
- **Cross-Store Inconsistency:** Store A updated, Store B still processing

**Mitigation:**
- **Write Acknowledgment Levels:**
  - **Metadata Only:** Return after KRaft commit (fast, but data not yet at stores)
  - **One Store:** Return after 1 store ACKs (balanced)
  - **All Stores:** Return after all stores ACK (slow, but fully consistent)
- **Client Caching:** Client caches pending writes, serves from cache until propagated
- **Version Tokens:** Return version token with write, clients include token in subsequent reads

=== 4.4. Bandwidth Costs at Scale

**Problem:** Federated architecture incurs **significant bandwidth costs** for cross-store communication.

**Bandwidth Analysis:**
[source,text]
----
Assumptions:
  - 100 federated stores
  - 1000 qps average load
  - Average query touches 3 stores
  - Average response size: 100 KB

Bandwidth required:
  - Queries: 1000 qps × 3 stores × 100 KB = 300 MB/s
  - Responses: 300 MB/s
  - Total: 600 MB/s = 4.8 Gbps

Monthly bandwidth:
  - 4.8 Gbps × 86400 sec/day × 30 days = 15.5 PB/month
  - At $0.10/GB egress: $1.5M/month bandwidth cost
----

**Cost Breakdown:**
- **Intra-Region:** Typically free or low-cost ($0.01/GB)
- **Cross-Region (Same Cloud):** $0.02-0.05/GB
- **Cross-Region (Different Clouds):** $0.08-0.15/GB
- **To Internet:** $0.10-0.20/GB

**Consequences:**
- **Cost Explosion:** Bandwidth becomes **dominant cost** at scale
- **Regional Affinity Critical:** Must minimize cross-region traffic
- **Economic Barrier:** Small institutions cannot afford federated participation

**Mitigation:**
- **Response Compression:** gzip/brotli compression (70% reduction)
- **Delta Encoding:** Send only changes, not full objects
- **Regional Hubs:** Deploy regional orchestrators to localize traffic
- **Peer-to-Peer:** Stores communicate directly (bypass orchestrator for large transfers)

== 5. Governance and Policy Challenges

=== 5.1. Multi-Stakeholder Governance

**Problem:** Federated mode involves **multiple independent institutions** with conflicting interests.

**Governance Conflicts:**
[cols="1,2,2"]
|===
|Issue |Institution A |Institution B

|**Data Retention**
|Retain indefinitely (research)
|Delete after 7 years (legal requirement)

|**Access Policies**
|Open access (public good)
|Restricted to .edu domains (institutional policy)

|**Monetization**
|Free tier only
|Charge $0.01/query (cost recovery)

|**Modality Support**
|All 6 modalities
|Graph and Document only (limited resources)

|**Schema Changes**
|Propose new metadata fields
|Resist changes (stability)
|===

**Coordination Overhead:**
- **Consensus Required:** Major changes need agreement from **majority of stores**
- **Veto Power:** Any store can refuse to adopt changes
- **Glacial Pace:** Months to approve simple changes

**Example:**
[source,text]
----
Proposal: Add "license" field to Hexad metadata

Timeline:
  - T=0:   Proposal submitted
  - T+30d: 10 stores approve, 5 neutral, 3 reject
  - T+60d: Negotiation with rejectors (concerns about schema bloat)
  - T+90d: Compromise: "license" optional, not required
  - T+120d: Implementation begins (phased rollout)
  - T+180d: Fully adopted

Result: 6 months for simple metadata addition
----

**Mitigation:**
- **Governance Framework:** Pre-defined voting procedures, quorum requirements
- **Lightweight Changes:** Allow non-breaking changes without full consensus
- **Opt-In Features:** New features optional, not mandatory
- **Advisory Board:** Elected representatives from major institutions

=== 5.2. Policy Synchronization and Drift

**Problem:** Access policies **diverge** across federated stores over time.

**Policy Drift Example:**
[source,text]
----
Initial: All stores enforce "Allow .edu domains only"

T+6 months:
  - Store A: Still enforcing original policy
  - Store B: Updated to "Allow .edu and .gov"
  - Store C: Reverted to "Public access" (new administration)

Query from @gmail.com:
  - Store A: ✗ Denied
  - Store B: ✗ Denied
  - Store C: ✓ Allowed

Result: Inconsistent authorization (confusing UX)
----

**Causes of Drift:**
- **Manual Updates:** Stores manually apply policy changes → prone to lag
- **Versioning:** Stores run different software versions with different policy engines
- **Intentional Divergence:** Store operators override global policies

**Consequences:**
- **Security Gaps:** Overly permissive stores bypass intended restrictions
- **Compliance Violations:** Some stores may violate institutional regulations
- **User Confusion:** "Works on one store, denied on another"

**Mitigation:**
- **Policy Registry:** Store policies in **global registry**, orchestrator validates before querying
- **Automated Enforcement:** Orchestrator **enforces policies** before forwarding queries (don't trust stores)
- **Policy Audit:** Regular audits to detect drift, flag violating stores
- **Policy Versioning:** Policies have version numbers, require minimum version

=== 5.3. Economic Incentives and Cost Sharing

**Problem:** Federated stores incur costs (compute, storage, bandwidth) → need **economic model** for sustainability.

**Cost Distribution:**
[cols="1,2,2,2"]
|===
|Store Type |Costs |Revenue Model |Sustainability

|**Academic**
|$10k/month (grant-funded)
|Free (public good)
|Depends on grant renewal

|**Commercial**
|$50k/month (production)
|Charge $0.01/query
|Profitable if >5M queries/month

|**Community**
|$500/month (volunteer)
|Donations
|Fragile (volunteers burn out)

|**Archive**
|$5k/month (shared infra)
|Institutional funding
|Stable but slow upgrades
|===

**Tragedy of the Commons:**
- Popular stores get **overloaded** (high query volume)
- Unpopular stores **idle** (wasted capacity)
- No mechanism to balance load or compensate popular stores

**Example:**
[source,text]
----
Archive.org hosts 80% of Document modality data
  - Receives 80% of federated queries
  - Bandwidth costs skyrocket
  - Archive.org considers de-federating to reduce costs

Result: Federated ecosystem at risk
----

**Mitigation:**
- **Reciprocity Agreements:** Stores agree to host each other's overflows
- **Payment Rails:** Integrate micropayments (e.g., Lightning Network, stablecoins)
- **Query Credits:** Each store gets N free queries/month, pays beyond that
- **Funding Pool:** Central fund (grants, donations) subsidizes high-cost stores

== 6. Data Sovereignty and Legal Challenges

=== 6.1. Cross-Jurisdictional Data Flows

**Problem:** Federated queries may **cross legal jurisdictions** with different data protection laws.

**Legal Frameworks:**
[cols="1,2,2"]
|===
|Jurisdiction |Law |Key Requirement

|**EU**
|GDPR
|Data cannot leave EU without adequacy decision

|**US**
|CLOUD Act
|US government can compel disclosure

|**China**
|Data Security Law
|Data localization (critical data stays in China)

|**Russia**
|Personal Data Law
|Russian citizens' data must be stored in Russia
|===

**Compliance Conflict:**
[source,text]
----
Scenario: EU user queries Hexad hosted across:
  - Store A (Germany): Subject to GDPR
  - Store B (US): Subject to CLOUD Act
  - Store C (China): Subject to Data Security Law

Problem:
  - GDPR: Cannot send personal data to China (no adequacy)
  - CLOUD Act: US government could compel Store B disclosure
  - China DSL: If data critical, cannot leave China

VeriSimDB must:
  1. Determine if query involves personal data
  2. Check user's jurisdiction
  3. Only query compliant stores
  4. Return partial results if necessary
----

**Consequences:**
- **Compliance Overhead:** Must track **data residency** for every Hexad
- **Partial Results:** May need to exclude stores for legal reasons
- **Audit Complexity:** Must prove compliance to regulators in multiple jurisdictions

**Mitigation:**
- **Data Tagging:** Tag Hexads with **jurisdiction metadata** (e.g., "EU-only", "US-only")
- **Query Planning:** Route queries only to **legally compliant stores**
- **Encryption:** Use **homomorphic encryption** to process data without exposing plaintext (but: performance cost)
- **Legal Counsel:** Consult lawyers specializing in cross-border data flows

=== 6.2. Right to be Forgotten (RTBF)

**Problem:** GDPR **Right to be Forgotten** requires deleting data across **all federated stores**.

**RTBF Challenge:**
[source,text]
----
User requests deletion of Hexad 550e8400-...

VeriSimDB must:
  1. Identify all stores hosting any modality of Hexad
     - Registry lookup: Stores [A, B, C, D, E]
  2. Send deletion request to each store
  3. Wait for confirmation (stores may take days to comply)
  4. Verify deletion (how to prove data gone?)
  5. Update registry to mark Hexad deleted

Complications:
  - Store B is offline (maintenance window)
  - Store C refuses (claims US CLOUD Act overrides GDPR)
  - Store D deleted data but has backup tapes (not yet purged)
  - Store E is a mirror (didn't receive deletion request)

Result: Incomplete deletion → GDPR violation
----

**Penalties:** GDPR fines up to **€20M or 4% of global revenue** (whichever higher)

**Mitigation:**
- **Deletion SLA:** Pre-negotiate **deletion timelines** with all stores (e.g., 30 days)
- **Immutable Flag:** Mark Hexad as "deleted" in registry immediately, lazy-delete from stores
- **Cryptographic Deletion:** Encrypt Hexads with **per-Hexad keys**, delete keys instead of data
- **Backup Coordination:** Require stores to certify backups also purged

=== 6.3. Sovereignty vs Availability Tradeoff

**Problem:** Strict data sovereignty requirements **limit query availability**.

**Example:**
[source,text]
----
Scenario: French research paper (Hexad X) hosted only on French stores

Global query from US user:
  - Option 1: Query French stores (slow, 150ms latency due to geography)
  - Option 2: Mirror to US stores (violates French data sovereignty laws)
  - Option 3: Deny access (poor UX)

Trade-off: Cannot optimize for both sovereignty AND performance
----

**Tension:**
- **Sovereignty:** Data must stay in jurisdiction → **limits geographic distribution**
- **Performance:** Low latency requires **data near users** → conflicts with sovereignty

**Mitigation:**
- **Edge Caching:** Cache **anonymized** versions near users (metadata only, no personal data)
- **Federated Learning:** Compute aggregates locally, only share aggregates (not raw data)
- **Legal Agreements:** Bilateral agreements between jurisdictions (expensive, slow)

== 7. Summary of Challenges

[cols="1,2,1"]
|===
|Challenge Category |Key Issues |Severity

|**Consensus**
|Raft quorum overhead, split-brain, leader election storms
|HIGH

|**Trust**
|Multi-party signatures, Byzantine faults, trust boundary explosion
|CRITICAL

|**Operations**
|Heterogeneous stores, cross-institutional SLAs, drift detection, observability gaps
|HIGH

|**Performance**
|Network latency dominates, fan-out amplification, write propagation delay, bandwidth costs
|HIGH

|**Governance**
|Multi-stakeholder conflicts, policy drift, economic incentives
|MEDIUM

|**Legal**
|Cross-jurisdictional flows, RTBF compliance, sovereignty vs availability
|CRITICAL
|===

== 8. When to Choose Federated Despite Challenges

Federated deployment is appropriate when:

1. **Data Sovereignty:** Institutional data MUST remain under institutional control (non-negotiable)
2. **Cross-Institutional Collaboration:** Multiple universities/labs need to share knowledge without centralization
3. **Compliance Requirements:** Legal/regulatory requirements prevent data centralization
4. **Open Science:** Commitment to decentralized, community-governed knowledge infrastructure
5. **Scale Beyond Single Institution:** Data volume exceeds what any single institution can host
6. **Resilience:** Need to survive single-institution failures (geographic redundancy)

For simpler use cases, **standalone** or **hybrid** modes are recommended.

== See Also

- link:deployment-modes.adoc[Deployment Modes Overview]
- link:challenges-standalone.adoc[Challenges: Standalone Mode]
- link:challenges-hybrid.adoc[Challenges: Hybrid Mode]
- link:technical-specification-kraft-metadata-log.adoc[KRaft Metadata Log Specification]
- link:zkp-and-sanctify-integration.adoc[Zero-Trust Security with ZKP]
- link:../WHITEPAPER.md[VeriSimDB White Paper]
